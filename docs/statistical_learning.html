<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>The Elements of Statistical Learning</title>

<script src="site_libs/header-attrs-2.23/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.0/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.0/css/v4-shims.min.css" rel="stylesheet" />
<link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
<link rel="manifest" href="favicon/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Book Club</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="writing_science.html">Writing Science</a>
</li>
<li>
  <a href="mastering_shiny.html">Mastering Shiny</a>
</li>
<li>
  <a href="statistical_learning.html">The Elements of Statistical Learning</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/loelschlaeger/book_club">
    <span class="fa fa-github"></span>
     
    Source
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">The Elements of Statistical Learning</h1>

</div>


<p><a href="https://hastie.su.domains/Papers/ESLII.pdf">Hastie,
Tibshirani, and Friedman (2009). The Elements of Statistical Learning.
Second Edition. Springer.</a></p>
<p><a href="statistical_learning.pdf">Download summary</a></p>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Statistical learning (learning from data) is important in science,
finance, industry and includes supervised learning (presence of outcome
variable for learning, prediction based on features) and unsupervised
learning (outcome variable is unobserved, identification of structure in
data).</p>
<p>Running examples in the book:</p>
<ul>
<li>Predicting whether an email is spam or not based on most occurring
words and punctuation marks</li>
<li>Explaining the amount of prostate specific antigen using cancer
volume, prostate weight, and other clinical measurements</li>
<li>Classification of handwritten digits based on grayscale maps</li>
<li>Clustering of DNA microarray data for cancer diagnostic</li>
</ul>
</div>
<div id="overview-of-supervised-learning" class="section level1"
number="2">
<h1><span class="header-section-number">2</span> Overview of Supervised
Learning</h1>
<p>Two simple but powerful prediction methods are least squares linear
models and <span class="math inline">\(k\)</span>-nearest neighbors. The
former makes huge assumptions about structure and yields stable but
possibly inaccurate predictions (low variance, high bias), the latter
makes very mild assumptions with often accurate but unstable predictions
(at least if <span class="math inline">\(k\)</span> is low, leading to
low bias and high variance).</p>
<p>Local methods like <span class="math inline">\(k\)</span>-nearest
neighbors suffer from the curse of dimensionality: in high dimensions,
samples only sparsely populate the input space and are close to an edge
(extrapolation might be required). By imposing restrictions on the model
class (e.g., linear models), this can be avoided. Many models have been
proposed that lie in the spectrum between rigid model assumptions and
flexibility, they will be presented in the book.</p>
</div>
<div id="linear-methods-for-regression" class="section level1"
number="3">
<h1><span class="header-section-number">3</span> Linear Methods for
Regression</h1>
<p>Linear regression models are simple and often adequate and
interpretable. The Gauss-Markov theorem states that the least squares
estimates have the smallest variance among all linear unbiased
estimates. However, it might be a good idea to trade a little bit bias
for a large reduction in variance. This is possible with different
variable subset selection and shrinkage methods:</p>
<ul>
<li>Best-subset selection: For each given size, find the best subset of
variables that minimizes the residual sum of squares. Strategies for
choosing the size will be discussed later. Not applicable for a large
number of variables.</li>
<li>Forward- and backward-stepwise selection: Searching through all
possible subsets quickly becomes infeasible. Instead, sequentially add
or remove variables.</li>
<li>Forward-stagewise regression: A more constrained version of
forward-stepwise, with benefits in high dimensions.</li>
<li>Ridge regression: The idea is to make the selection process
continuous by shrinking the coefficient values, which can further reduce
variance. The penalty for coefficient sizes is quadratic (<span
class="math inline">\(L_2\)</span>). Variables with small variance are
shrunk the most.</li>
<li>The lasso (least absolute shrinkage and selection operator): Similar
to ridge regression, but the penalty is in absolute coefficient value
(<span class="math inline">\(L_1\)</span>). This can make some
coefficients to be exactly zero.</li>
<li>Elastic-net penalty: A convex combination of the ridge and the lasso
penalty.</li>
<li>Least angle regression: Similar to forward-stepwise regression with
the difference, that entering variables are not fit completely but only
until it no longer has the highest correlation with the current
residual. A slight modification provides an efficient way of computing
the entire lasso path.</li>
<li>Principal components regression: Use transformed variables with
large sample variance.</li>
<li>Partial least squares: Also constructs a set of linear combinations
of the inputs for regression, but unlike principal components regression
it uses the dependent variable for construction.</li>
</ul>
</div>
<div id="linear-methods-for-classification" class="section level1"
number="4">
<h1><span class="header-section-number">4</span> Linear Methods for
Classification</h1>
<p>The goal is to classify inputs into a finite number of categories.
This means dividing the input space into a collection of regions. Linear
here means that the regions are separated by linear boundaries.</p>
<ul>
<li>One option is linear regression of inputs to indicators of the
response. When there is a large number of classes, classes can be masked
by others. That means, that the predicted regression value never
dominates. This can be avoided by adding polynomial terms to the
regression equation.</li>
<li>Another option is linear discriminant analysis, where each class
density is modeled as a multivariate Gaussian with constant error
covariance. With two classes, this is the same as linear regression of
the class indicators. If the error covariance is not constant but
class-dependent, this yields quadratic discriminant functions. The
difference in covariances can be regularized. In the linear case, since
only the relative differences to the class centroids matter, the data
can be projected in a subspace of dimension at most number of classes
minus 1, which can be a significant drop in dimension. This subspace can
be further decomposed in term of centroid separation. By choosing an
optimal subspace dimension, this projection can also be used for
classification.</li>
<li>Logistic regression updates the regression approach by ensuring that
the dependent variables are proper probabilities. The lasso penalty can
be used for variable selection.</li>
<li>Separating hyperplane classifiers construct linear decision
boundaries that explicitly try to separate the data into different
classes as well as possible by minimizing the distance of misclassified
points to the decision boundary. The optimal separating hyperplane
maximizes the distance to the closest point from either class, which
provides a unique solution with generalizes better.</li>
</ul>
</div>
<div id="basis-expansions-and-regularization" class="section level1"
number="5">
<h1><span class="header-section-number">5</span> Basis Expansions and
Regularization</h1>
<p>To transcend the constraints of linear models, one can enhance the
input vector by applying transformations to it and then utilize linear
models in the resulting expanded input space. One approach is to use
piecewise polynomials, also known as splines, which involve dividing the
input domain into contiguous intervals and fitting a separate polynomial
in each interval. However, it is crucial to determine the appropriate
polynomial order, number of knots, and their placement. The B-spline
basis is a convenient way to represent them numerically. By using a
maximal set of knots and employing regularization, the knot selection
problem can be avoided, and complexity can be controlled. Another
alternative is to use wavelets, which use a complete orthonormal basis
to represent the function, but selectively shrink and choose the
coefficients for a sparse representation. This technique is particularly
useful for signal compression.</p>
</div>
<div id="kernel-smoothing-methods" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Kernel Smoothing
Methods</h1>
<p>For greater flexibility, one can fit a different, yet simple model at
each query point separately by utilizing data from the closest
observations. To ensure that the resulting regression function is
smooth, one can apply kernels that assign weights to observations that
decay smoothly with distance from the target point. These kernels are
typically parameterized to dictate the width of the neighborhood;
examples include the Epanechnikov kernel, tri-cube kernel, or Gaussian
density (with non-compact support). To avoid bias on the boundary of the
domain due to asymmetry of the kernel in that region, one can fit
straight lines or higher-order polynomials instead of constants.
However, there is a trade-off between reducing bias and increasing
variance. The kernel smoothing technique generalizes naturally to
multiple dimensions, although boundary effects become a more significant
issue. Additionally, kernel density estimation is an unsupervised
learning procedure that can be utilized for classification.</p>
</div>
<div id="model-assessment-and-selection" class="section level1"
number="7">
<h1><span class="header-section-number">7</span> Model Assessment and
Selection</h1>
<p>Models generalize well if they have a good prediction capability on
independent test data. To decide between competing models, a reasonable
criterium is their generalization.</p>
<p>The prediction error of a model has three sources: error in the data
generating process which we cannot avoid, squared bias which is the
amount by which the average of our estimate differs from the true
parameter, and the variance of our estimates from the truth for
different data. Typically, the more complex the model, the lower the
bias, but the higher the variance.</p>
<p>Prediction error, however, always depends on a specific loss function
and can be behave differently for, e.g., the squared-error loss versus
the 0-1 loss.</p>
<p>Prediction error on the training sample is not a good estimate of the
generalization error, since it consistently decreases with model
complexity. However, a model with zero training error is overfit to the
training data and will typically generalize poorly.</p>
<p>We need another method for estimating the expected test error for a
model. Typically, a model has a tuning parameter, and we seek to find
the optimal parameter.</p>
<p>In a data-rich situation, one approach is dividing the data into
training, validation, and test set (rule of thumb is 50%, 25%, 25%). But
for insufficient amount of data, this does not work.</p>
<p>One option are information criteria. The AIC estimates the in-sample
prediction error, assuming a log-likelihood loss function. One
ingredient are the number of model parameters. This concept needs to be
generalized to the effective number of parameters in case of
regularization. Another option is the BIC, which penalizes model
complexity by the factor <span class="math inline">\(\log(N)\)</span>
and is motivated from a Bayesian approach of model selection, namely
Bayes factors. There is no clear choice between AIC and BIC for model
selection: BIC is consistent but chooses too simple models in the finite
sample case.</p>
<p>Cross-validation</p>
<p>Bootstrap</p>
</div>
<div id="model-inference-and-averaging" class="section level1"
number="8">
<h1><span class="header-section-number">8</span> Model Inference and
Averaging</h1>
</div>
<div id="additive-models-trees-and-related-methods"
class="section level1" number="9">
<h1><span class="header-section-number">9</span> Additive Models, Trees,
and Related Methods</h1>
</div>
<div id="boosting-and-additive-trees" class="section level1"
number="10">
<h1><span class="header-section-number">10</span> Boosting and Additive
Trees</h1>
</div>
<div id="neural-networks" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Neural Networks</h1>
</div>
<div id="support-vector-machines-and-flexible-discriminants"
class="section level1" number="12">
<h1><span class="header-section-number">12</span> Support Vector
Machines and Flexible Discriminants</h1>
</div>
<div id="prototype-methods-and-nearest-neighbors" class="section level1"
number="13">
<h1><span class="header-section-number">13</span> Prototype Methods and
Nearest-Neighbors</h1>
</div>
<div id="unsupervised-learning" class="section level1" number="14">
<h1><span class="header-section-number">14</span> Unsupervised
Learning</h1>
</div>
<div id="random-forests" class="section level1" number="15">
<h1><span class="header-section-number">15</span> Random Forests</h1>
<p>A random forest is an average of trees. The hope is to reduce
variance. The bias in the average remains the same. Trees are
high-variance and low-bias procedures, so this averaging of trees fitted
to bootstrap-sampled versions of the training data (bagging) is
beneficial. The averaging reduces more variance if the trees are
uncorrelated, so the idea for random forests is also to reduce the
correlation between the single trees. This is achieved by random
selection of input variables in the tree-growing process. We must be
careful because this procedure increases the variance again.</p>
</div>
<div id="ensemble-learning" class="section level1" number="16">
<h1><span class="header-section-number">16</span> Ensemble Learning</h1>
<p>Ensemble learning means to build a prediction model by combining the
strengths of a collection of simpler models (e.g., random forest).</p>
</div>
<div id="undirected-graphical-models" class="section level1"
number="17">
<h1><span class="header-section-number">17</span> Undirected Graphical
Models</h1>
</div>
<div id="high-dimensional-problems" class="section level1" number="18">
<h1><span class="header-section-number">18</span> High-Dimensional
Problems</h1>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
