<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>The Elements of Statistical Learning</title>

<script src="site_libs/header-attrs-2.20/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
<link rel="manifest" href="favicon/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Book Club</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="writing_science.html">Writing Science</a>
</li>
<li>
  <a href="mastering_shiny.html">Mastering Shiny</a>
</li>
<li>
  <a href="statistical_learning.html">The Elements of Statistical Learning</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/loelschlaeger/book_club">
    <span class="fa fa-github"></span>
     
    Source
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">The Elements of Statistical Learning</h1>

</div>


<p><a href="https://hastie.su.domains/Papers/ESLII.pdf">Hastie,
Tibshirani, and Friedman (2009). The Elements of Statistical Learning.
Second Edition. Springer.</a></p>
<p><a href="statistical_learning.pdf">Download summary</a></p>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Statistical learning plays a key role in science, finance, industry,
and many more areas. This book is about learning from data: supervised
learning (presence of outcome variable for learning, the focus of this
book) and unsupervised learning (outcome variable is unobserved).</p>
<p>Running examples:</p>
<ul>
<li>Classification of spam emails</li>
<li>Explaining prostate specific antigen from clinical measurements via
regression</li>
<li>Classification of handwritten digits</li>
<li>Clustering of DNA microarray data for cancer diagnostic</li>
</ul>
</div>
<div id="overview-of-supervised-learning" class="section level1"
number="2">
<h1><span class="header-section-number">2</span> Overview of Supervised
Learning</h1>
<p>Two simple but powerful prediction methods are least squares linear
models and <span class="math inline">\(k\)</span>-nearest neighbors. The
former makes huge assumptions about structure and yields stable but
possibly inaccurate predictions (low variance, high bias), the latter
makes very mild assumptions with often accurate but unstable predictions
(at least if <span class="math inline">\(k\)</span> is low, leading to
low bias and high variance).</p>
<p>Local methods like <span class="math inline">\(k\)</span>-nearest
neighbors suffer from the curse of dimensionality: in high dimensions,
samples only sparsely populate the input space and are close to an edge
(extrapolation might be required). By imposing restrictions on the model
class (e.g., linear models), this can be avoided. Many models have been
proposed that lie in the spectrum between rigid model assumptions and
flexibility, they will be presented in the book.</p>
</div>
<div id="linear-methods-for-regression" class="section level1"
number="3">
<h1><span class="header-section-number">3</span> Linear Methods for
Regression</h1>
<p>Linear regression models are simple and often adequate and
interpretable. The Gauss-Markov theorem states that the least squares
estimates have the smallest variance among all linear unbiased
estimates. However, it might be a good idea to trade a little bit bias
for a large reduction in variance. This is possible with different
variable subset selection and shrinkage methods:</p>
<ul>
<li>Best-subset selection: For each given size, find the best subset of
variables that minimizes the residual sum of squares. Strategies for
choosing the size will be discussed later. Not applicable for a large
number of variables.</li>
<li>Forward- and backward-stepwise selection: Searching through all
possible subsets quickly becomes infeasible. Instead, sequentially add
or remove variables.</li>
<li>Forward-stagewise regression: A more constrained version of
forward-stepwise, with benefits in high dimensions.</li>
<li>Ridge regression: The idea is to make the selection process
continuous by shrinking the coefficient values, which can further reduce
variance. The penalty for coefficient sizes is quadratic (<span
class="math inline">\(L_2\)</span>). Variables with small variance are
shrunk the most.</li>
<li>The lasso (least absolute shrinkage and selection operator): Similar
to ridge regression, but the penalty is in absolute coefficient value
(<span class="math inline">\(L_1\)</span>). This can make some
coefficients to be exactly zero.</li>
<li>Elastic-net penalty: A convex combination of the ridge and the lasso
penalty.</li>
<li>Least angle regression: Similar to forward-stepwise regression with
the difference, that entering variables are not fit completely but only
until it no longer has the highest correlation with the current
residual. A slight modification provides an efficient way of computing
the entire lasso path.</li>
<li>Principal components regression: Use transformed variables with
large sample variance.</li>
<li>Partial least squares: Also constructs a set of linear combinations
of the inputs for regression, but unlike principal components regression
it uses the dependent variable for construction.</li>
</ul>
</div>
<div id="linear-methods-for-classification" class="section level1"
number="4">
<h1><span class="header-section-number">4</span> Linear Methods for
Classification</h1>
<p>The goal is to classify inputs into a finite number of categories.
This means dividing the input space into a collection of regions. Linear
here means that the regions are separated by linear boundaries.</p>
<ul>
<li>One option is linear regression of inputs to indicators of the
response. When there is a large number of classes, classes can be masked
by others. That means, that the predicted regression value never
dominates. This can be avoided by adding polynomial terms to the
regression equation.</li>
<li>Another option is linear discriminant analysis, where each class
density is modeled as a multivariate Gaussian with constant error
covariance. With two classes, this is the same as linear regression of
the class indicators. If the error covariance is not constant but
class-dependent, this yields quadratic discriminant functions. The
difference in covariances can be regularized. In the linear case, since
only the relative differences to the class centroids matter, the data
can be projected in a subspace of dimension at most number of classes
minus 1, which can be a significant drop in dimension. This subspace can
be further decomposed in term of centroid separation. By choosing an
optimal subspace dimension, this projection can also be used for
classification.</li>
<li>Logistic regression updates the regression approach by ensuring that
the dependent variables are proper probabilities. The lasso penalty can
be used for variable selection.</li>
<li>Separating hyperplane classifiers construct linear decision
boundaries that explicitly try to separate the data into different
classes as well as possible by minimizing the distance of misclassified
points to the decision boundary. The optimal separating hyperplane
maximizes the distance to the closest point from either class, which
provides a unique solution with generalizes better.</li>
</ul>
</div>
<div id="basis-expansions-and-regularization" class="section level1"
number="5">
<h1><span class="header-section-number">5</span> Basis Expansions and
Regularization</h1>
<p>To transcend the constraints of linear models, one can enhance the
input vector by applying transformations to it and then utilize linear
models in the resulting expanded input space. One approach is to use
piecewise polynomials, also known as splines, which involve dividing the
input domain into contiguous intervals and fitting a separate polynomial
in each interval. However, it is crucial to determine the appropriate
polynomial order, number of knots, and their placement. The B-spline
basis is a convenient way to represent them numerically. By using a
maximal set of knots and employing regularization, the knot selection
problem can be avoided, and complexity can be controlled. Another
alternative is to use wavelets, which use a complete orthonormal basis
to represent the function, but selectively shrink and choose the
coefficients for a sparse representation. This technique is particularly
useful for signal compression.</p>
</div>
<div id="kernel-smoothing-methods" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Kernel Smoothing
Methods</h1>
<p>For greater flexibility, one can fit a different, yet simple model at
each query point separately by utilizing data from the closest
observations. To ensure that the resulting regression function is
smooth, one can apply kernels that assign weights to observations that
decay smoothly with distance from the target point. These kernels are
typically parameterized to dictate the width of the neighborhood;
examples include the Epanechnikov kernel, tri-cube kernel, or Gaussian
density (with non-compact support). To avoid bias on the boundary of the
domain due to asymmetry of the kernel in that region, one can fit
straight lines or higher-order polynomials instead of constants.
However, there is a trade-off between reducing bias and increasing
variance. The kernel smoothing technique generalizes naturally to
multiple dimensions, although boundary effects become a more significant
issue. Additionally, kernel density estimation is an unsupervised
learning procedure that can be utilized for classification.</p>
</div>
<div id="model-assessment-and-selection" class="section level1"
number="7">
<h1><span class="header-section-number">7</span> Model Assessment and
Selection</h1>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
