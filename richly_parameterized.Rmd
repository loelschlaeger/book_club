---
title: "Richly Parameterized Linear Models"
output:
  pdf_document:
    number_sections: true
    latex_engine: pdflatex
---

[Hodges (2014). Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects.](https://www.taylorfrancis.com/books/mono/10.1201/b16019/richly-parameterized-linear-models-james-hodges)

```{r, eval=knitr::is_html_output(), echo=FALSE}
knitr::asis_output('[Download summary](richly_parameterized.pdf)')
```

# An Opinionated Survey of Methods for Mixed Linear Models

The general notation of mixed linear models is $y = X\beta + Zu + \epsilon$, where the observation $y$ is explained through a design matrix $X$ connected to fixed effects $\beta$, a design matrix $Z$ connected to normal random effects $u$, and a normally distributed error $\epsilon$. The covariance matrix of the random effects $u$ (the errors $\epsilon$) is a function of unknowns $\phi_G$ ($\phi_R$). "All of the oddities and inconveniences examined in this book arise because $\phi_G$ is unknown." The author distinguishes between old-style (the underlying distribution is of interest) versus new-style random effects (also the levels themselves are of interest). Three examples are given, which can be reproduced in `R` with [`{lme4}`](https://cran.r-project.org/web/packages/lme4/index.html). Estimation results have some oddities (non-convergence, zero variance estimates), probably due to non-identified models, too less data points, or numerical problems.

Estimation with conventional analysis 1. estimates $\phi$ (via maximizing the restricted likelihood to avoid bias) and 2. treats $\phi$ as known and estimates $\beta$ and $u$ (via maximizing the likelihood). This approach has some problems ... while Bayesian analysis alleviates some (?) problems.

# Two More Tools: Alternative Formulation, Measures of Complexity

# Penalized Splines as Mixed Linear Models

# Additive Models and Models with Interactions




